{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c01257e",
   "metadata": {},
   "source": [
    "# Adversarial Attacks with ResNet-34 (Updated)\n",
    "\n",
    "This notebook performs two tasks on a 100-class subset of ImageNet-1K:\n",
    "1. Evaluate clean top-1/top-5 accuracy using a pretrained ResNet-34.\n",
    "2. Generate adversarial examples using FGSM (ε=0.02), re-evaluate accuracy, and visualize misclassifications with human-readable labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb63ecb",
   "metadata": {},
   "source": [
    "## 1. Imports and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33a7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json, random\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "# Normalization constants\n",
    "mean_norms = np.array([0.485, 0.456, 0.406])\n",
    "std_norms  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "plain_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_norms, std=std_norms),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde9913",
   "metadata": {},
   "source": [
    "## 2. Task 1: Clean Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318ecf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean Eval: 100%|██████████| 4/4 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Top-1 accuracy: 76.00%\n",
      "Clean Top-5 accuracy: 94.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=\"TestDataSet\",\n",
    "    transform=plain_transforms\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load mapping for 401–500 indices\n",
    "with open(\"TestDataSet/labels_list.json\") as f:\n",
    "    entries = json.load(f)\n",
    "idx_to_true = {i: int(entries[i].split(\":\",1)[0]) for i in range(len(entries))}\n",
    "\n",
    "# Load pretrained ResNet-34\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model.eval().to(device)\n",
    "\n",
    "# Evaluate clean accuracy\n",
    "top1 = top5 = total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(loader, desc=\"Clean Eval\"):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        _, p5 = logits.topk(5, dim=1)\n",
    "        true = torch.tensor([idx_to_true[int(l)] for l in labels], device=p5.device)\n",
    "        top1 += (p5[:,0] == true).sum().item()\n",
    "        top5 += (p5 == true.unsqueeze(1)).any(dim=1).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Clean Top-1 accuracy: {top1/total*100:.2f}%\")\n",
    "print(f\"Clean Top-5 accuracy: {top5/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd81ca0",
   "metadata": {},
   "source": [
    "## 3. Task 2: FGSM Attack & Adversarial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694b504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FGSM: 100%|██████████| 500/500 [00:03<00:00, 164.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial top-1: 6.20%\n",
      "Adversarial top-5: 35.40%\n"
     ]
    }
   ],
   "source": [
    "# FGSM helper\n",
    "cn = torch.tensor(mean_norms, device=device)[:,None,None]\n",
    "cs = torch.tensor(std_norms,  device=device)[:,None,None]\n",
    "min_val = (0 - cn) / cs\n",
    "max_val = (1 - cn) / cs\n",
    "\n",
    "def fgsm(image, eps, grad):\n",
    "    return torch.max(torch.min(image + eps*grad.sign(), max_val), min_val)\n",
    "\n",
    "# Single-image loader\n",
    "si_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Run FGSM\n",
    "epsilon = 0.02\n",
    "adv_images, adv_labels, orig_preds, adv_preds = [], [], [], []\n",
    "\n",
    "for img, lab in tqdm(si_loader, desc=\"FGSM\"):\n",
    "    img = img.to(device).detach().requires_grad_(True)\n",
    "    lab = lab.to(device)\n",
    "    out = model(img)\n",
    "    true_idx = torch.tensor([idx_to_true[int(lab)]], device=device)\n",
    "    loss = F.cross_entropy(out, true_idx)\n",
    "    model.zero_grad(); loss.backward()\n",
    "    adv = fgsm(img, epsilon, img.grad.data).detach().to(torch.float32)\n",
    "    adv_images.append(adv.squeeze(0).cpu())\n",
    "    adv_labels.append(int(lab))\n",
    "    orig_preds.append(out.argmax(1).item())\n",
    "    adv_preds.append(model(adv).argmax(1).item())\n",
    "\n",
    "# Build adversarial set\n",
    "adv_tensor = torch.stack(adv_images)\n",
    "lab_tensor = torch.tensor(adv_labels)\n",
    "adv_set = TensorDataset(adv_tensor, lab_tensor)\n",
    "adv_loader = DataLoader(adv_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate adversarial set\n",
    "top1 = top5 = total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in adv_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        out = model(imgs)\n",
    "        _, p5 = out.topk(5,1)\n",
    "        true = torch.tensor([idx_to_true[int(l)] for l in labs], device=p5.device)\n",
    "        top1 += (p5[:,0]==true).sum().item()\n",
    "        top5 += (p5==true.unsqueeze(1)).any(1).sum().item()\n",
    "        total += labs.size(0)\n",
    "print(f\"Adversarial top-1: {top1/total*100:.2f}%\")\n",
    "print(f\"Adversarial top-5: {top5/total*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0a4ad",
   "metadata": {},
   "source": [
    "## 4. Visualization of Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edfdf80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m true_name = label_401_to_500[true_idx]\n\u001b[32m     28\u001b[39m adv_name = all_labels[adv_idx]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[43morig_images\u001b[49m[i], adv_images[i]]):\n\u001b[32m     31\u001b[39m     ax = axes[row, col]\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# im = inv_norm(img).permute(1, 2, 0).clamp(0, 1).numpy()\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'orig_images' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare label maps for human-readable names\n",
    "weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "all_labels = weights.meta['categories']\n",
    "label_401_to_500 = {i: all_labels[i] for i in range(401, 501)}\n",
    "\n",
    "# Find 3 examples where clean was correct but adv is misclassified\n",
    "picked = []\n",
    "for i, (o, a, l) in enumerate(zip(orig_preds, adv_preds, adv_labels)):\n",
    "    true_img_idx = idx_to_true[l]\n",
    "    if o == true_img_idx and a != true_img_idx:\n",
    "        picked.append(i)\n",
    "    if len(picked) >= 5:\n",
    "        break\n",
    "\n",
    "# Un-normalizer\n",
    "inv_norm = transforms.Normalize(\n",
    "    mean=(-mean_norms / std_norms).tolist(),\n",
    "    std=(1 / std_norms).tolist()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "for col, i in enumerate(picked):\n",
    "    true_idx = idx_to_true[adv_labels[i]]\n",
    "    adv_idx = adv_preds[i]\n",
    "\n",
    "    true_name = label_401_to_500[true_idx]\n",
    "    adv_name = all_labels[adv_idx]\n",
    "\n",
    "    for row, img in enumerate([orig_images[i], adv_images[i]]):\n",
    "        ax = axes[row, col]\n",
    "        # im = inv_norm(img).permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "        im = inv_norm(img).permute(1, 2, 0).clamp(0, 1).detach().cpu().numpy()\n",
    "\n",
    "        ax.imshow(im)\n",
    "        ax.axis('off')\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"orig\\ntrue={true_name}\")\n",
    "        else:\n",
    "            ax.set_title(f\"adv\\npred={adv_name}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077835f7",
   "metadata": {},
   "source": [
    "# Task3:  Improved attacks （PGD Without Target Attacks）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62eee2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean   Top‑1  76.00%   Top‑5  94.20%\n",
      "PGD     Top‑1   0.00%   Top‑5   2.60%\n",
      "\n",
      "Saved Adversarial Test Set 2 → adv_set_pgd.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "eps   = 0.02\n",
    "steps = 10\n",
    "\n",
    "args = {\n",
    "    \"data\":      \"TestDataSet\",\n",
    "    \"labels\":    \"TestDataSet/labels_list.json\",\n",
    "    \"eps\":       eps,\n",
    "    \"steps\":     steps,\n",
    "    \"alpha\":     eps / steps,   \n",
    "    \"batch\":     128,\n",
    "    \"workers\":   8,\n",
    "    \"rand_start\": True,\n",
    "    \"out\":       \"adv_set_pgd.pt\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- ImageNet normalization -------------------\n",
    "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "\n",
    "eps_n   = torch.tensor(args[\"eps\"]   / std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "alpha_n = torch.tensor(args[\"alpha\"] / std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "std_tensor = torch.tensor(std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "\n",
    "min_val = torch.tensor(((0 - mean) / std).reshape(3,1,1), device=device)\n",
    "max_val = torch.tensor(((1 - mean) / std).reshape(3,1,1), device=device)\n",
    "\n",
    "# ------------------- PGD Attack Function -------------------\n",
    "def pgd_attack(model, x, y, eps_n, alpha_n, steps, min_v, max_v, rand_start):\n",
    "    if rand_start:\n",
    "        x_adv = x + torch.empty_like(x).uniform_(-1, 1) * eps_n\n",
    "        x_adv = torch.clamp(x_adv, min_v, max_v)\n",
    "    else:\n",
    "        x_adv = x.clone()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv = x_adv.detach().requires_grad_(True)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        F.cross_entropy(model(x_adv), y).backward()\n",
    "        grad_sign = x_adv.grad.sign()\n",
    "        x_adv = x_adv + alpha_n * grad_sign\n",
    "        x_adv = torch.clamp(x_adv, x - eps_n, x + eps_n)\n",
    "        x_adv = torch.clamp(x_adv, min_v, max_v)\n",
    "    return x_adv.detach()\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "dataset   = datasets.ImageFolder(root=args[\"data\"], transform=transform)\n",
    "loader    = DataLoader(dataset, batch_size=args[\"batch\"], shuffle=False, num_workers=args[\"workers\"])\n",
    "\n",
    "with open(args[\"labels\"]) as f:\n",
    "    idx2true = {i: int(e.split(\":\",1)[0]) for i, e in enumerate(json.load(f))}\n",
    "\n",
    "# ------------------- Load Pretrained Model -------------------\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Evaluation Function -------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, idx2true, device, tag=\"\"):\n",
    "    top1 = top5 = tot = 0\n",
    "    for x, labs in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        labs = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "        logits = model(x)\n",
    "        top1 += (logits.argmax(1) == labs).sum().item()\n",
    "        top5 += (logits.topk(5, 1)[1] == labs[:, None]).any(1).sum().item()\n",
    "        tot  += labs.size(0)\n",
    "    print(f\"{tag:<8}Top‑1 {top1/tot*100:6.2f}%   Top‑5 {top5/tot*100:6.2f}%\")\n",
    "    return top1 / tot, top5 / tot\n",
    "\n",
    "# ------------------- Evaluate Clean Accuracy -------------------\n",
    "evaluate(model, loader, idx2true, device, \"Clean\")\n",
    "\n",
    "# ------------------- PGD Attack Loop -------------------\n",
    "adv_batches, lab_batches = [], []\n",
    "for x, labs in loader:\n",
    "    x = x.to(device, non_blocking=True)\n",
    "    y = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "\n",
    "    adv = pgd_attack(model, x, y, eps_n, alpha_n,\n",
    "                     args[\"steps\"], min_val, max_val, args[\"rand_start\"])\n",
    "    \n",
    "    raw_delta = ((adv - x).abs() * std_tensor).max().item()\n",
    "    assert raw_delta <= args[\"eps\"] + 1e-6, f\"ε-constraint {raw_delta:.6f} > ε\"\n",
    "\n",
    "    adv_batches.append(adv)\n",
    "    lab_batches.append(labs.to(device))\n",
    "\n",
    "adv_tensor = torch.cat(adv_batches)\n",
    "lab_tensor = torch.cat(lab_batches)\n",
    "\n",
    "# ------------------- Evaluate PGD Accuracy -------------------\n",
    "pgd_loader = DataLoader(TensorDataset(adv_tensor, lab_tensor),\n",
    "                        batch_size=args[\"batch\"])\n",
    "evaluate(model, pgd_loader, idx2true, device, \"PGD\")\n",
    "\n",
    "# ------------------- Save Dataset -------------------\n",
    "torch.save({\"images\": adv_tensor.cpu(), \"labels\": lab_tensor.cpu()}, args[\"out\"])\n",
    "print(f\"\\nSaved Adversarial Test Set 2 → {args['out']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean   Top‑1  76.00%   Top‑5  94.20%\n",
      "TargetedTop‑1   0.80%   Top‑5   8.00%\n",
      "\n",
      "Saved Adversarial Test Set 2 → adv_set_targeted.pt\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# ------------------- Parameters -------------------\n",
    "args = {\n",
    "    \"data\": \"TestDataSet\",\n",
    "    \"labels\": \"TestDataSet/labels_list.json\",\n",
    "    \"eps\": 0.02,\n",
    "    \"steps\": 5,           # 比 FGSM 更强\n",
    "    \"alpha\": 0.004,\n",
    "    \"batch\": 64,\n",
    "    \"workers\": 4,\n",
    "    \"target_class\": 0,     # 所有图像强制扰动成这个类 (tench)\n",
    "    \"out\": \"adv_set_targeted.pt\"\n",
    "}\n",
    "\n",
    "# ------------------- Device -------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "\n",
    "# ------------------- ImageNet normalization -------------------\n",
    "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "\n",
    "eps_n   = torch.tensor(args[\"eps\"]   / std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "alpha_n = torch.tensor(args[\"alpha\"] / std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "std_tensor = torch.tensor(std, device=device, dtype=torch.float32).view(3,1,1)\n",
    "\n",
    "min_val = torch.tensor(((0 - mean) / std).reshape(3,1,1), device=device)\n",
    "max_val = torch.tensor(((1 - mean) / std).reshape(3,1,1), device=device)\n",
    "\n",
    "# ------------------- Targeted PGD Function -------------------\n",
    "def targeted_pgd(model, x, target, eps_n, alpha_n, steps, min_v, max_v):\n",
    "    x_adv = x.clone()\n",
    "    for _ in range(steps):\n",
    "        x_adv = x_adv.detach().requires_grad_(True)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss = F.cross_entropy(model(x_adv), target)\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.sign()\n",
    "        x_adv = x_adv - alpha_n * grad\n",
    "        x_adv = torch.clamp(x_adv, x - eps_n, x + eps_n)\n",
    "        x_adv = torch.clamp(x_adv, min_v, max_v)\n",
    "    return x_adv.detach()\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "dataset   = datasets.ImageFolder(root=args[\"data\"], transform=transform)\n",
    "loader    = DataLoader(dataset, batch_size=args[\"batch\"], shuffle=False, num_workers=args[\"workers\"])\n",
    "\n",
    "with open(args[\"labels\"]) as f:\n",
    "    idx2true = {i: int(e.split(\":\",1)[0]) for i, e in enumerate(json.load(f))}\n",
    "\n",
    "# ------------------- Load Pretrained Model -------------------\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ------------------- Evaluation Function -------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, idx2true, device, tag=\"\"):\n",
    "    top1 = top5 = tot = 0\n",
    "    for x, labs in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        labs = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "        logits = model(x)\n",
    "        top1 += (logits.argmax(1) == labs).sum().item()\n",
    "        top5 += (logits.topk(5, 1)[1] == labs[:, None]).any(1).sum().item()\n",
    "        tot  += labs.size(0)\n",
    "    print(f\"{tag:<8}Top‑1 {top1/tot*100:6.2f}%   Top‑5 {top5/tot*100:6.2f}%\")\n",
    "    return top1 / tot, top5 / tot\n",
    "\n",
    "# ------------------- Clean Accuracy -------------------\n",
    "evaluate(model, loader, idx2true, device, \"Clean\")\n",
    "\n",
    "# ------------------- Targeted Attack Loop -------------------\n",
    "adv_batches, lab_batches = [], []\n",
    "for x, labs in loader:\n",
    "    x = x.to(device)\n",
    "    target = torch.full_like(labs, fill_value=args[\"target_class\"], device=device)\n",
    "\n",
    "    adv = targeted_pgd(model, x, target, eps_n, alpha_n,\n",
    "                       args[\"steps\"], min_val, max_val)\n",
    "\n",
    "    # Ensure constraint\n",
    "    raw_delta = ((adv - x).abs() * std_tensor).max().item()\n",
    "    assert raw_delta <= args[\"eps\"] + 1e-6, f\"ε-constraint {raw_delta:.6f} > ε\"\n",
    "\n",
    "    adv_batches.append(adv)\n",
    "    lab_batches.append(labs.to(device))\n",
    "\n",
    "adv_tensor = torch.cat(adv_batches)\n",
    "lab_tensor = torch.cat(lab_batches)\n",
    "\n",
    "# ------------------- Evaluate Targeted PGD Accuracy -------------------\n",
    "adv_loader = DataLoader(TensorDataset(adv_tensor, lab_tensor), batch_size=args[\"batch\"])\n",
    "evaluate(model, adv_loader, idx2true, device, \"Targeted\")\n",
    "\n",
    "# ------------------- Save Dataset -------------------\n",
    "torch.save({\"images\": adv_tensor.cpu(), \"labels\": lab_tensor.cpu()}, args[\"out\"])\n",
    "print(f\"\\nSaved Adversarial Test Set 2 → {args['out']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63979639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean     Top‑1  76.00%   Top‑5  94.20%\n",
      "Patch‑PGD Top‑1  31.60%   Top‑5  67.60%\n",
      "\n",
      "Saved Adversarial Test Set 3 → adv_set_patch_soft.pt\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ───────────── Parameters ─────────────\n",
    "args = {\n",
    "    \"data\": \"./TestDataSet\",\n",
    "    \"labels\": \"./TestDataSet/labels_list.json\",\n",
    "    \"eps\": 0.3,\n",
    "    \"steps\": 80,\n",
    "    \"alpha\": 0.006,\n",
    "    \"batch\": 16,\n",
    "    \"workers\": 4,\n",
    "    \"target\": 0,\n",
    "    \"out\": \"adv_set_patch_soft.pt\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "\n",
    "# ───────────── Normalization ─────────────\n",
    "mean = np.array([0.485, 0.456, 0.406], np.float32)\n",
    "std  = np.array([0.229, 0.224, 0.225], np.float32)\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "\n",
    "eps_n   = torch.tensor(args[\"eps\"]   / std, device=device).view(3,1,1)\n",
    "alpha_n = torch.tensor(args[\"alpha\"] / std, device=device).view(3,1,1)\n",
    "min_v   = torch.tensor(((0 - mean)/std).reshape(3,1,1), device=device)\n",
    "max_v   = torch.tensor(((1 - mean)/std).reshape(3,1,1), device=device)\n",
    "std_t   = torch.tensor(std, device=device).view(3,1,1)\n",
    "\n",
    "# ───────────── Gaussian blur ─────────────\n",
    "def gaussian_blur(mask, k=15, sigma=6):\n",
    "    coords = torch.arange(k, dtype=torch.float32, device=mask.device) - (k - 1) / 2\n",
    "    kernel = torch.exp(-(coords**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    kernel = kernel.view(1, 1, 1, k)\n",
    "    mask = F.conv2d(mask, kernel, padding=(0, k // 2), groups=1)\n",
    "    mask = F.conv2d(mask, kernel.transpose(2, 3), padding=(k // 2, 0), groups=1)\n",
    "    return mask\n",
    "\n",
    "# ───────────── Attack Function ─────────────\n",
    "def patch_pgd(model, x, tgt, mask, eps_n, alpha_n, steps, min_v, max_v):\n",
    "    x_adv = x.clone()\n",
    "    mom = torch.zeros_like(x)\n",
    "    beta = 0.9\n",
    "    for _ in range(steps):\n",
    "        x_adv = x_adv.detach().requires_grad_(True)\n",
    "        loss = F.cross_entropy(model(x_adv), tgt)\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.sign() * mask\n",
    "        mom = beta * mom + grad\n",
    "        x_adv = x_adv - alpha_n * mom.sign()\n",
    "        x_adv = torch.max(torch.min(x_adv, x + eps_n * mask), x - eps_n * mask)\n",
    "        x_adv = torch.clamp(x_adv, min_v, max_v)\n",
    "    return x_adv.detach()\n",
    "\n",
    "# ───────────── Dataset + Loader ─────────────\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "dataset = datasets.ImageFolder(root=args[\"data\"], transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=args[\"batch\"], shuffle=False, num_workers=args[\"workers\"])\n",
    "\n",
    "with open(args[\"labels\"]) as f:\n",
    "    idx2true = {i: int(e.split(\":\",1)[0]) for i, e in enumerate(json.load(f))}\n",
    "\n",
    "# ───────────── Model ─────────────\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ───────────── Accuracy Function ─────────────\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, idx2true, device, tag=\"\"):\n",
    "    t1 = t5 = n = 0\n",
    "    for x, labs in loader:\n",
    "        x = x.to(device)\n",
    "        labs = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "        logits = model(x)\n",
    "        t1 += (logits.argmax(1) == labs).sum().item()\n",
    "        t5 += (logits.topk(5, 1)[1] == labs[:, None]).any(1).sum().item()\n",
    "        n += labs.size(0)\n",
    "    print(f\"{tag:<10}Top‑1 {t1/n*100:6.2f}%   Top‑5 {t5/n*100:6.2f}%\")\n",
    "\n",
    "# ───────────── Clean Evaluation ─────────────\n",
    "evaluate(model, loader, idx2true, device, \"Clean\")\n",
    "\n",
    "# ───────────── Attack Loop ─────────────\n",
    "adv_batches, lab_batches = [], []\n",
    "for x, labs in loader:\n",
    "    x = x.to(device)\n",
    "    B, C, H, W = x.shape\n",
    "\n",
    "    hard = torch.zeros(B, 1, H, W, device=device)\n",
    "    for i in range(B):\n",
    "        top = random.randint(0, H - 32)\n",
    "        left = random.randint(0, W - 32)\n",
    "        hard[i, :, top:top+32, left:left+32] = 1.0\n",
    "    mask = gaussian_blur(hard, k=15, sigma=6).clamp(0, 1)\n",
    "    mask = mask.repeat(1, 3, 1, 1)\n",
    "\n",
    "    tgt = torch.full((B,), args[\"target\"], device=device, dtype=torch.long)\n",
    "\n",
    "    adv = patch_pgd(model, x, tgt, mask, eps_n, alpha_n, args[\"steps\"], min_v, max_v)\n",
    "\n",
    "    raw = ((adv - x).abs() * std_t * (mask > 0.5)).max().item()\n",
    "    assert raw <= args[\"eps\"] + 1e-6\n",
    "\n",
    "    adv_batches.append(adv)\n",
    "    lab_batches.append(labs.to(device))\n",
    "\n",
    "# ───────────── Save & Eval ─────────────\n",
    "adv_tensor = torch.cat(adv_batches)\n",
    "lab_tensor = torch.cat(lab_batches)\n",
    "\n",
    "pgd_loader = DataLoader(TensorDataset(adv_tensor, lab_tensor), batch_size=args[\"batch\"])\n",
    "evaluate(model, pgd_loader, idx2true, device, \"Patch‑PGD\")\n",
    "\n",
    "torch.save({\"images\": adv_tensor.cpu(), \"labels\": lab_tensor.cpu()}, args[\"out\"])\n",
    "print(f\"\\nSaved Adversarial Test Set 3 → {args['out']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb4f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean     Top‑1  76.00%   Top‑5  94.20%\n",
      "Patch‑PGD Top‑1   6.60%   Top‑5  41.40%\n",
      "\n",
      "Saved Adversarial Test Set 3 → adv_set_patch_soft_nontarget.pt\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# ────── Config ──────\n",
    "args = {\n",
    "    \"data\": \"./TestDataSet\",\n",
    "    \"labels\": \"./TestDataSet/labels_list.json\",\n",
    "    \"eps\": 0.3,\n",
    "    \"steps\": 80,\n",
    "    \"alpha\": 0.006,\n",
    "    \"batch\": 16,\n",
    "    \"workers\": 4,\n",
    "    \"out\": \"adv_set_patch_soft_nontarget.pt\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
    "\n",
    "# ────── Normalization ──────\n",
    "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "\n",
    "eps_n   = torch.tensor(args[\"eps\"]   / std, device=device).view(3,1,1)\n",
    "alpha_n = torch.tensor(args[\"alpha\"] / std, device=device).view(3,1,1)\n",
    "min_v   = torch.tensor(((0 - mean)/std).reshape(3,1,1), device=device)\n",
    "max_v   = torch.tensor(((1 - mean)/std).reshape(3,1,1), device=device)\n",
    "std_t   = torch.tensor(std, device=device).view(3,1,1)\n",
    "\n",
    "# ────── Gaussian Blur ──────\n",
    "def gaussian_blur(mask, k=15, sigma=6):\n",
    "    coords = torch.arange(k, dtype=torch.float32, device=mask.device) - (k - 1) / 2\n",
    "    kernel = torch.exp(-(coords**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    kernel = kernel.view(1, 1, 1, k)\n",
    "    mask = F.conv2d(mask, kernel, padding=(0, k // 2), groups=1)\n",
    "    mask = F.conv2d(mask, kernel.transpose(2, 3), padding=(k // 2, 0), groups=1)\n",
    "    return mask\n",
    "\n",
    "# ────── Non-Targeted PGD ──────\n",
    "def patch_pgd(model, x, y, mask, eps_n, alpha_n, steps, min_v, max_v):\n",
    "    x_adv = x.clone()\n",
    "    mom = torch.zeros_like(x)\n",
    "    beta = 0.9\n",
    "    for _ in range(steps):\n",
    "        x_adv = x_adv.detach().requires_grad_(True)\n",
    "        loss = F.cross_entropy(model(x_adv), y)\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.sign() * mask\n",
    "        mom = beta * mom + grad\n",
    "        x_adv = x_adv + alpha_n * mom.sign()  # ← untargeted: maximize loss\n",
    "        x_adv = torch.max(torch.min(x_adv, x + eps_n * mask), x - eps_n * mask)\n",
    "        x_adv = torch.clamp(x_adv, min_v, max_v)\n",
    "    return x_adv.detach()\n",
    "\n",
    "# ────── Dataset & Loader ──────\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "dataset = datasets.ImageFolder(root=args[\"data\"], transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=args[\"batch\"], shuffle=False, num_workers=args[\"workers\"])\n",
    "\n",
    "with open(args[\"labels\"]) as f:\n",
    "    idx2true = {i: int(e.split(\":\",1)[0]) for i, e in enumerate(json.load(f))}\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, idx2true, device, tag=\"\"):\n",
    "    t1 = t5 = n = 0\n",
    "    for x, labs in loader:\n",
    "        x = x.to(device)\n",
    "        labs = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "        logits = model(x)\n",
    "        t1 += (logits.argmax(1) == labs).sum().item()\n",
    "        t5 += (logits.topk(5, 1)[1] == labs[:, None]).any(1).sum().item()\n",
    "        n += labs.size(0)\n",
    "    print(f\"{tag:<10}Top‑1 {t1/n*100:6.2f}%   Top‑5 {t5/n*100:6.2f}%\")\n",
    "\n",
    "# ────── Clean Accuracy ──────\n",
    "evaluate(model, loader, idx2true, device, \"Clean\")\n",
    "\n",
    "# ────── Adversarial Generation ──────\n",
    "adv_batches, lab_batches = [], []\n",
    "for x, labs in loader:\n",
    "    x = x.to(device)\n",
    "    y = torch.tensor([idx2true[int(l)] for l in labs], device=device)\n",
    "    B, C, H, W = x.shape\n",
    "\n",
    "    hard = torch.zeros(B, 1, H, W, device=device)\n",
    "    for i in range(B):\n",
    "        top = random.randint(0, H - 32)\n",
    "        left = random.randint(0, W - 32)\n",
    "        hard[i, :, top:top+32, left:left+32] = 1.0\n",
    "    mask = gaussian_blur(hard).clamp(0, 1).repeat(1, 3, 1, 1)\n",
    "\n",
    "    adv = patch_pgd(model, x, y, mask, eps_n, alpha_n, args[\"steps\"], min_v, max_v)\n",
    "\n",
    "    raw = ((adv - x).abs() * std_t * (mask > 0.5)).max().item()\n",
    "    assert raw <= args[\"eps\"] + 1e-6\n",
    "\n",
    "    adv_batches.append(adv)\n",
    "    lab_batches.append(labs.to(device))\n",
    "\n",
    "adv_tensor = torch.cat(adv_batches)\n",
    "lab_tensor = torch.cat(lab_batches)\n",
    "\n",
    "evaluate(model,\n",
    "         DataLoader(TensorDataset(adv_tensor, lab_tensor), batch_size=args[\"batch\"]),\n",
    "         idx2true, device, \"Patch‑PGD\")\n",
    "\n",
    "torch.save({\"images\": adv_tensor.cpu(), \"labels\": lab_tensor.cpu()}, args[\"out\"])\n",
    "print(f\"\\nSaved Adversarial Test Set 3 → {args['out']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
